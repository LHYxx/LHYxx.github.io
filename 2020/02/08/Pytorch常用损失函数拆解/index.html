<!DOCTYPE html>
<html lang="zh-cmn-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content>
    <link rel="shortcut icon" href="/img/book.png">
    <title>Pytorch常用损失函数拆解-小新xx</title>
    
        
            <link href="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet">
            <link href="https://cdn.bootcss.com/academicons/1.8.6/css/academicons.min.css" rel="stylesheet">
            <link href="https://cdn.bootcss.com/font-awesome/5.9.0/css/all.min.css" rel="stylesheet">
            <link href="https://cdn.bootcss.com/animate.css/3.7.2/animate.min.css" rel="stylesheet">
        
    
    <link rel="stylesheet" href="/css/adagio.css">

    <style type="text/css">
        .jumbotron{
            background: url();
            background-repeat: no-repeat;
            background-size: 100% 100%;
        }
        .tag-list{
            font-weight: bold;
            font-size: 200;
            color: red;
        }
    </style>

<!-- 下面是对mathjax的设置 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      linebreaks: {automatic: ['\\']}
    },
    "HTML-CSS": { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true } }
  });
</script>



</head>
<body>
    <div class="container-fluid">
    <nav class="nav">
        <div class="collapse navbar-collapse" id="navbar-sm">
            
            
            <div class="navbar-nav">
                <a href="/index.html" class="nav-item nav-link">主页</a>
            </div>
            
            <div class="navbar-nav">
                <a href="/others/aboutme.html" class="nav-item nav-link">LHYxx</a>
            </div>
            
        </div>
    </nav>
</div>

<div class="d-flex d-md-none" style="width: 100%; background-color: #e9ecef">
    <div class="nav">
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-sm"
            aria-expanded="false" aria-label="Toggle Navigation">
            <i class="fas fa-bars fa-lg"></i>
        </button>
    </div>
    <nav class="navbar ml-auto">
        <a class="navbar-brand" href="/">
            
            LHYxx
            
        </a>
    </nav>
</div>


<div class="container d-none d-md-block my-navbar">
    <nav class="navbar navbar-expand-sm navbar-light bg-transparent">
        <a class="navbar-brand " href="/">
            
            LHYxx
            
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
            aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
                
                <li class="nav-item pl-2 pr-2 ">
                    <a class="nav-link" href="/index.html">主页</a>
                </li>
                
                <li class="nav-item pl-2 pr-2 ">
                    <a class="nav-link" href="/others/aboutme.html">LHYxx</a>
                </li>
                
            </ul>
        </div>
    </nav>
</div>




    <div class="jumbotron jumbotron-fluid">
<!-- <div class="jumbotron jumbotron-fluid" style="background-image: url()"> -->
    <div class="container" >
        
        <h1 class="mt-4 article-title page-title" style="font-weight: bold; color: black">Pytorch常用损失函数拆解</h1>
        
        <p class="lead text-gray mt-3" style="color: black">By 小新; Published on 2020-02-08</p>
        
        <div class="tags">
            <ul class="tag-list">
                
                <li class="tag-list-item">
                    <a class="tag-list-link" href="/tags/pytorch/">pytorch</a>
                </li>
                
                <li class="tag-list-item">
                    <a class="tag-list-link" href="/tags/损失函数/">损失函数</a>
                </li>
                
            </ul>
        </div>
        
    </div>
</div>
    <div class="container">
        <div class="row">
            <div class="col-md-9 pt-2">
                <div class="row">
                    <div class="col-12">
                        <main>
                            <article class="article-text page-content"><h1 id="Pytorch常用损失函数拆解"><a href="#Pytorch常用损失函数拆解" class="headerlink" title="Pytorch常用损失函数拆解"></a>Pytorch常用损失函数拆解</h1><p>本文从理论和实践两方面来全面梳理一下常用的损失函数。（避免自己总是一瓶子不满半瓶子晃荡……）。要么理论满分，编码时不会用；要么编码是会调包，但是不明白其中的计算原理。本文来科普一下。<br>我们将每个损失函数分别从理论和pytorch中的实现两个方面来拆解一下。</p>
<p>另外，解释一下torch.nn.Module 和 torch.nn.functional(俗称F)中损失函数的区别。<br>Module的损失函数例如CrossEntropyLoss、NLLLoss等是封装之后的损失函数类，是一个类，因此其中的变量可以自动维护。经常是对F中的函数的封装。<br>而F中的损失函数只是单纯的函数。<br>当然我们也可以自己构造自己的损失函数对象。有时候损失函数并不需要太复杂，没有必要特意封装一个类，直接调用F中的函数也是可以的。使用哪种看具体实现需求而定。</p>
<h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><p>交叉熵损失，是分类任务中最常用的一个损失函数。</p>
<h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><p>直接上理论公式 ：<br>$$<br>Loss(\hat{x}, x)=-\sum_{i=1}^{n} x\log (\hat{x})<br>$$<br>其中$x$是真实标签，$\hat{x}$是预测的<strong>类分布（通常是使用softmax将模型输出转换为概率分布）</strong>，也就是$x$与$\hat{x}$中的元素分别表示对应类别的概率。<br>举个例子，清晰明了</p>
<blockquote>
<p>x=[0, 1, 0]  # 假设该样本属于第二类<br>$\hat{x}$=[0.1, 0.5, 0.4]  # 因为是分布，所以属于各个类的和为1<br>$Loss(\hat{x}, x) = -0 \times log(0.1) - 1 \times log(0.5) - 0 \times log(0.4) = log(0.5)$</p>
</blockquote>
<h3 id="pytorch-实现"><a href="#pytorch-实现" class="headerlink" title="pytorch 实现"></a>pytorch 实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch.nn import CrossEntropyLoss</span><br></pre></td></tr></table></figure>

<p>举例<br>实际使用中需要注意几点</p>
<ul>
<li><p><code>torch.nn.CrossEntropyLoss(input, target)</code>中的标签<code>target</code>使用的不是one-hot形式，而是类别的序号。形如 <code>target = [1, 3, 2]</code> 表示3个样本分别属于第1类、第3类、第2类。</p>
</li>
<li><p><code>torch.nn.CrossEntropyLoss(input, target)</code>的<code>input</code>是<strong>没有归一化的每个类的得分</strong>，而不是softmax之后的分布。</p>
</li>
</ul>
<p>举例，输入的形式大概就像相面这种格式：<br>$target=[1, 3, 2]$<br>$input=[[0.13, -0.18, 0.87],[0.25, -0.04, 0.32],[0.24, -0.54, 0.53]]$</p>
<p>然后就将他们扔到CrossEntropyLoss函数中，就可以得到损失。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = CrossEntropyLoss(input, target)</span><br></pre></td></tr></table></figure>

<p>我们看CrossEntropyLoss函数里面的实现，是下面这样子的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, input, target):</span><br><span class="line">    return F.cross_entropy(input, target, weight=self.weight,</span><br><span class="line">                           ignore_index=self.ignore_index, reduction=self.reduction)</span><br></pre></td></tr></table></figure>

<p>是调用的torch.nn.functional（俗称F）中的cross_entropy()函数。<br>参数</p>
<ul>
<li>input：预测值，（batch，dim），这里dim就是要分类的<strong>总类别数</strong></li>
<li>target：真实值，（batch），这里为啥是1维的？<strong>因为真实值并不是用one-hot形式表示，而是直接传类别id。</strong></li>
<li>weight：指定权重，（dim），可选参数，可以给每个类指定一个权重。通常在训练数据中不同类别的样本数量差别较大时，可以使用权重来平衡。</li>
<li>ignore_index：指定忽略一个真实值，（int），也就是手动忽略一个真实值。</li>
<li>reduction：在[none, mean, sum]中选，string型。none表示不降维，返回和target相同形状；mean表示对一个batch的损失求均值；sum表示对一个batch的损失求和。</li>
</ul>
<p>其中参数weight、ignore_index、reduction要在实例化CrossEntropyLoss对象时指定,例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = torch.nn.CrossEntropyLoss(reduction=&apos;none&apos;)</span><br></pre></td></tr></table></figure>

<p>我们再看一下<strong>F中的cross_entropy的实现</strong>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return nll_loss(log_softmax(input, dim=1), target, weight, None, ignore_index, None, reduction)</span><br></pre></td></tr></table></figure>

<p>可以看到就是先调用<code>log_softmax</code>,再调用<code>nll_loss</code>。<br><code>log_softmax</code>就是先<code>softmax</code>再取<code>log</code>：<br>$$<br>log_softmax(x) = \log({softmax(x)})<br>$$</p>
<p><code>nll_loss</code>是negative log likelihood loss：<br>详细介绍见下面<strong>torch.nn.NLLLoss</strong>，计算公式如下：</p>
<p>$$<br>nll_loss(\hat{x}, class) = -\hat{x}[class]<br>$$</p>
<p>例如假设$\hat{x}=[1, 2, 3], class=2$，则$nll_loss(\hat{x}, class)=-\hat{x}[class]=-\hat{x}[2]=-2$</p>
<p>源码中给了个用法例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># input is of size N x C = 3 x 5</span><br><span class="line">input = torch.randn(3, 5, requires_grad=True)</span><br><span class="line"># each element in target has to have 0 &lt;= value &lt; C</span><br><span class="line">target = torch.tensor([1, 0, 4])</span><br><span class="line">output = F.nll_loss(F.log_softmax(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure>

<p>因此，其实CrossEntropyLoss损失，就是<strong>softmax + log + nll_loss</strong>的集成。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CrossEntropyLoss(input, target) = nll_loss(log_softmax(input, dim=1), target)</span><br></pre></td></tr></table></figure>

<p>CrossEntropyLoss中的target必须是LongTensor类型。<br>实验如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([[<span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">target = torch.LongTensor([<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">loss_fun = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">loss = loss_fun(pred, target)  </span><br><span class="line">print(loss)  <span class="comment"># 输出为tensor(1.3133)</span></span><br><span class="line">loss2 = F.nll_loss(F.log_softmax(pred, dim=<span class="number">1</span>), target)</span><br><span class="line">print(loss2)  <span class="comment"># 输出为tensor(1.3133)</span></span><br></pre></td></tr></table></figure>

<p>数学形式就是<br>$$<br>Loss(input, target) = -\log{\frac{exp(input[target])}{\Sigma_j{exp(input[j])}}}<br>$$</p>
<h2 id="torch-nn-BCELoss"><a href="#torch-nn-BCELoss" class="headerlink" title="torch.nn.BCELoss"></a>torch.nn.BCELoss</h2><h3 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h3><p>CrossEntropy损失函数适用于总共有N个类别的分类。<strong>当N=2时，即二分类任务，只需要判断是还是否的情况，就可以使用二分类交叉熵损失：BCELoss</strong><br>二分类交叉熵损失。上公式 <strong>（y是真实标签，x是预测值）</strong>：<br>$$<br>loss(x, y) = -\frac{1}{n} \Sigma_{i}({y_{i}\times log(x_{i})+(1-y_{i})\times log(1-x_{i}))}<br>$$<br>其实<strong>这个函数就是CrossEntropyLoss的当类别数N=2时候的特例</strong>。因为类别数为2，属于第一类的概率为y，那么属于第二类的概率自然就是(1-y)。因此套用与CrossEntropy损失的计算方法，用对应的标签乘以对应的预测值再求和，就得到了最终的损失。</p>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BCELoss(x, y)</span><br></pre></td></tr></table></figure>

<p>x形状（batch，*），y形状与x相同。<br>x与y中每个元素，表示的是该维度上属于（或不属于）这个类的概率。<br>另外，pytorch中的BCELoss可以为每个类指定权重。通常，当训练数据中正例和反例的比例差别较大时，可以为其赋予不同的权重，weight的形状应该是一个一维的，元素的个数等于类别数。</p>
<p>实际使用如下例，计算BCELoss(pred, target)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([<span class="number">0.4</span>, <span class="number">0.1</span>])  <span class="comment"># 可以理解为第一个元素分类为是的概率为0.4，第二个元素分类为是的概率为0.1。</span></span><br><span class="line">target = torch.FloatTensor([<span class="number">0.2</span>, <span class="number">0.8</span>])  <span class="comment"># 实际上第一个元素分类为是的概率为0.2，第二个元素分类为是的概率为0.8。</span></span><br><span class="line">loss_fun = nn.BCELoss(reduction=<span class="string">'mean'</span>)  <span class="comment"># reduction可选 none, sum, mean, batchmean</span></span><br><span class="line">loss = loss_fun(pred, target)</span><br><span class="line">print(loss)  <span class="comment"># tensor(1.2275)</span></span><br><span class="line"></span><br><span class="line">a = -(<span class="number">0.2</span> * np.log(<span class="number">0.4</span>) + <span class="number">0.8</span> * np.log(<span class="number">0.6</span>) + <span class="number">0.8</span> * np.log(<span class="number">0.1</span>) + <span class="number">0.2</span> * np.log(<span class="number">0.9</span>))/<span class="number">2</span></span><br><span class="line">print(a)  <span class="comment"># 1.2275294114572126</span></span><br></pre></td></tr></table></figure>

<p>可以看到，计算BCELoss(pred, target)与上面理论中的公式一样。</p>
<h4 id="内部实现"><a href="#内部实现" class="headerlink" title="内部实现"></a>内部实现</h4><p>pytorch 中的<code>torch.nn.BCELoss</code>类，实际上就是调用了<code>F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)</code></p>
<h2 id="torch-nn-BCEWithLogitsLoss"><a href="#torch-nn-BCEWithLogitsLoss" class="headerlink" title="torch.nn.BCEWithLogitsLoss"></a>torch.nn.BCEWithLogitsLoss</h2><h3 id="理论-2"><a href="#理论-2" class="headerlink" title="理论"></a>理论</h3><p>该函数实际上与BCELoss相同，只是BCELoss的输入x，在输入之前需要先手动经过sigmoid激活函数映射到（0， 1）区间，而<strong>该函数将sigmoid与BCELoss整合到一起了</strong>。<br>也就是先将输入经过sigmoid函数，然后计算BCE损失。</p>
<h3 id="实践-1"><a href="#实践-1" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BCEWithLogitsLoss(x, y)</span><br></pre></td></tr></table></figure>

<p>x与y的形状要求与BCELoss相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([<span class="number">0.4</span>, <span class="number">0.1</span>])</span><br><span class="line">target = torch.FloatTensor([<span class="number">0.2</span>, <span class="number">0.8</span>])</span><br><span class="line">loss_fun = nn.BCEWithLogitsLoss(reduction=<span class="string">'mean'</span>)  <span class="comment"># reduction可选 none, sum, mean, batchmean</span></span><br><span class="line">loss = loss_fun(pred, target)</span><br><span class="line">print(loss)  <span class="comment"># tensor(0.7487)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面的过程与下面的过程结果相同</span></span><br><span class="line">loss_fun = nn.BCELoss(reduction=<span class="string">'mean'</span>)  <span class="comment"># reduction可选 none, sum, mean, batchmean</span></span><br><span class="line">loss = loss_fun(torch.sigmoid(pred), target)  <span class="comment"># 先经过sigmoid，然后与target计算BCELoss</span></span><br><span class="line">print(loss)  <span class="comment"># tensor(0.7487)</span></span><br></pre></td></tr></table></figure>

<p>可以看出，先对输入pred调用sigmoid，在调用BCELoss，结果就等于直接调用BCEWithLogitsLoss。</p>
<h2 id="torch-nn-L1Loss"><a href="#torch-nn-L1Loss" class="headerlink" title="torch.nn.L1Loss"></a>torch.nn.L1Loss</h2><h3 id="理论-3"><a href="#理论-3" class="headerlink" title="理论"></a>理论</h3><p>L1损失很简单，公式如下：<br>$$<br>loss(x, y)=\frac{1}{n}\Sigma_{i} {|x_{i}-y_{i}|}<br>$$<br>x是预测值，y是真实值。</p>
<h3 id="实践-2"><a href="#实践-2" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.L1Loss(x, y)</span><br></pre></td></tr></table></figure>

<p>x形状：任意形状<br>y形状：与输入形状相同</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([[3, 1], [1, 0]])</span><br><span class="line">target = torch.FloatTensor([[1, 0], [1, 0]])</span><br><span class="line">loss_fun = nn.L1Loss()</span><br><span class="line">loss = loss_fun(pred, target)</span><br><span class="line">print(loss)  # tensor(0.7500)</span><br></pre></td></tr></table></figure>

<p>其中L1Loss的内部实现为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, input, target):</span><br><span class="line">    return F.l1_loss(input, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure>

<p>我们可以看到，其实还是对F.l1_loss的封装。</p>
<h2 id="torch-nn-MSELoss"><a href="#torch-nn-MSELoss" class="headerlink" title="torch.nn.MSELoss"></a>torch.nn.MSELoss</h2><h3 id="理论-4"><a href="#理论-4" class="headerlink" title="理论"></a>理论</h3><p>L1Loss可以理解为向量的1-范数，MSE均方误差就可以理解为向量的2-范数，或矩阵的F-范数。<br>$$<br>loss(x, y) = \frac{1}{n} \Sigma(x_{i} - y_{i})^{2}<br>$$<br>x是预测值，y是真实值。</p>
<h3 id="实践-3"><a href="#实践-3" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MSELoss(x, y)</span><br></pre></td></tr></table></figure>

<p>x任意形状，y与x形状相同。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([[3, 1], [1, 0]])</span><br><span class="line">target = torch.FloatTensor([[1, 0], [1, 0]])</span><br><span class="line">loss_fun = nn.MSELoss()</span><br><span class="line">loss = loss_fun(pred, target)</span><br><span class="line">print(loss)  # tensor(1.2500)</span><br></pre></td></tr></table></figure>

<p>其中MSELoss内部实现为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, input, target):</span><br><span class="line">    return F.mse_loss(input, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure>

<p>本质上是对F中mse_loss函数的封装。</p>
<h2 id="torch-nn-NLLLoss"><a href="#torch-nn-NLLLoss" class="headerlink" title="torch.nn.NLLLoss"></a>torch.nn.NLLLoss</h2><h3 id="理论-5"><a href="#理论-5" class="headerlink" title="理论"></a>理论</h3><p>NLLLoss（Negative Log Likelihood Loss），其数学表达形式为：<br>$$<br>loss(x, y) = -log(x[y])<br>$$<br>前面讲到CrossEntropyLoss中用的nll_loss，实际上，该损失函数就是对<code>F.nll_loss</code>的封装，功能也和<code>nll_loss</code>相同。<br>正如前面所说，先把输入x进行<code>softmax</code>，在进行<code>log</code>，再输入该函数中就是<code>CrossEntropyLoss</code>。</p>
<h3 id="实践-4"><a href="#实践-4" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.NLLLoss(x, y)</span><br></pre></td></tr></table></figure>

<p>x是预测值，形状为（batch，dim）<br>y是真实值，形状为（batch）<br>形状要求与CrossEntropyLoss相同。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([[3, 1], [2, 4]])</span><br><span class="line">target = torch.LongTensor([0, 1])  #target必须是Long型</span><br><span class="line">loss_fun = nn.NLLLoss()</span><br><span class="line">loss = loss_fun(pred, target)</span><br><span class="line">print(loss)  # tensor(-3.5000)</span><br></pre></td></tr></table></figure>

<p>其内部实现实际上就是调用了F.nll_loss()：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, input, target):</span><br><span class="line">    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)</span><br></pre></td></tr></table></figure>

<h2 id="torch-nn-KLDivLoss"><a href="#torch-nn-KLDivLoss" class="headerlink" title="torch.nn.KLDivLoss"></a>torch.nn.KLDivLoss</h2><h3 id="理论-6"><a href="#理论-6" class="headerlink" title="理论"></a>理论</h3><p>KL散度通常用来衡量两个连续分布之间的距离。两个分布越相似，KL散度越接近0。<br>KL散度又叫相对熵，具体理论可以参考<a href="https://lhyxx.top/2019/09/15/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E7%86%B5/">之前的信息论基础一文</a><br>$$<br>loss(x, y) = \frac{1}{n}\Sigma_{i}({x_{i}\times\log{\frac{x_{i}}{y_{i}}}})<br>$$<br>注意，这里 x 与 y 都是分布，分布就意味着其中所有元素求和概率为1。<br>例如，$x=[0.1, 0.2, 0.7], y=[0.5, 0.2, 0.3]$<br>则$loss(x, y) = \frac{1}{3}\times(0.1\times\log{\frac{0.1}{0.5}} + 0.2\times\log{\frac{0.2}{0.2}} + 0.7\times\log{\frac{0.7}{0.3}})=0.43216$<br>本例中计算的log都是以e为底的。</p>
<h3 id="实践-5"><a href="#实践-5" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.KLDivLoss(input, target)</span><br></pre></td></tr></table></figure>

<p>试验测试<code>torch.nn.KLDivLoss</code>，计算<code>KL(pred|target)</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.7</span>])</span><br><span class="line">target = torch.FloatTensor([<span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">loss_fun = nn.KLDivLoss(reduction=<span class="string">'sum'</span>)  <span class="comment"># reduction可选 none, sum, mean, batchmean</span></span><br><span class="line">loss = loss_fun(target.log(), pred)</span><br><span class="line">print(loss)  <span class="comment"># tensor(0.4322)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#上面的计算过程等价于下面</span></span><br><span class="line">a = (<span class="number">0.1</span> * np.log(<span class="number">1</span>/<span class="number">5</span>) + <span class="number">0.2</span> * np.log(<span class="number">1</span>) + <span class="number">0.7</span> * np.log(<span class="number">7</span>/<span class="number">3</span>))</span><br><span class="line">print(a)  <span class="comment"># 0.43216</span></span><br></pre></td></tr></table></figure>

<p><code>input</code>应该是log-probabilities，<code>target</code>是probabilities。<code>input</code>和<code>target</code>形状相同。<br>该函数是对<code>F.kl_div(input, target, reduction=self.reduction)</code>的封装。其原型为：<br><code>torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></p>
<p><strong>注意</strong>，使用<code>nn.KLDivLoss</code>计算<code>KL(pred|target)</code>时，需要将<code>pred</code>和<code>target</code>调换位置，而且<code>target</code>需要先取对数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_fun(target.log(), pred)</span><br></pre></td></tr></table></figure>

</article>
                        </main>
                        
                        
                    </div>
                </div>
                <div class="row mt-5 mb-5">
                    <div class="col-12">
                        <div class="row">
    <div class="col">
        <nav aria-label="paginator" class="paginator">
            <ul class="pagination d-none d-md-flex pagination-lg justify-content-center">
                <li class="page-item ">
                    <a class="page-link"
                        href="/2020/02/11/pytorch-optimizer与学习率/"
                        aria-label="Previous">
                        <span aria-hidden="true">&laquo;
                            pytorch|optimizer与学习率</span>
                        <span class="sr-only">Previous</span>
                    </a>
                </li>
                <li class="page-item ">
                    <a class="page-link"
                        href="/2019/11/30/配置cuda和pytorch-gpu/"
                        aria-label="Next">
                        <span
                            aria-hidden="true">配置cuda和pytorch-gpu
                            &raquo;</span>
                        <span class="sr-only">Next</span>
                    </a>
                </li>
            </ul>
            <ul class="pagination d-md-none justify-content-center">
                <li class="page-item ">
                    <a class="page-link"
                        href="/2020/02/11/pytorch-optimizer与学习率/"
                        aria-label="Previous">
                        <span aria-hidden="true">&laquo;
                            pytorch|optimizer与学习率</span>
                        <span class="sr-only">Previous</span>
                    </a>
                </li>
                <li class="page-item ">
                    <a class="page-link"
                        href="/2019/11/30/配置cuda和pytorch-gpu/"
                        aria-label="Next">
                        <span
                            aria-hidden="true">配置cuda和pytorch-gpu
                            &raquo;</span>
                        <span class="sr-only">Next</span>
                    </a>
                </li>
            </ul>
        </nav>
    </div>
</div>



                    </div>
                </div>
                <div class="row">
                    <div class="col-12">
                        <div id="vcomment"></div>
                    </div>
                </div>
            </div>
            <div class="col-md-3">
                <div class="container pt-4 page-sidebar">
                    
                    <div class="row">
    <div class="col">
        <h6>APPLAUSE FOR ME</h6>
        <div id="applause-easy"></div>
    </div>
</div>
                    
                    <hr class="row">
                    <div class="row toc-container">
                        <div class="col-12">
                            <h6>NAVIGATION</h6>
                            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch常用损失函数拆解"><span class="toc-text">Pytorch常用损失函数拆解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CrossEntropyLoss"><span class="toc-text">CrossEntropyLoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch-实现"><span class="toc-text">pytorch 实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-BCELoss"><span class="toc-text">torch.nn.BCELoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论-1"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实践"><span class="toc-text">实践</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#内部实现"><span class="toc-text">内部实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-BCEWithLogitsLoss"><span class="toc-text">torch.nn.BCEWithLogitsLoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论-2"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实践-1"><span class="toc-text">实践</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-L1Loss"><span class="toc-text">torch.nn.L1Loss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论-3"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实践-2"><span class="toc-text">实践</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-MSELoss"><span class="toc-text">torch.nn.MSELoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论-4"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实践-3"><span class="toc-text">实践</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-NLLLoss"><span class="toc-text">torch.nn.NLLLoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论-5"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实践-4"><span class="toc-text">实践</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-KLDivLoss"><span class="toc-text">torch.nn.KLDivLoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论-6"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实践-5"><span class="toc-text">实践</span></a></li></ol></li></ol></li></ol>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <footer>
    <div class="jumbotron jumbotron-fluid mb-0">
        <div class="container-fluid">
            <div class="col text-center">
                <div class="bottom-social">
                    <div class="row">
    <div class="col text-center">
        <ul class="list-inline">
            
            <li class="list-inline-item">
                
                <a href="https://github.com/LHYxx">
                    <span class="fa-stack fa-2x icon-link">
                        <i class="fas fa-circle fa-stack-2x"></i>
                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                
            </li>
            
            <li class="list-inline-item">
                
                <a href="https://www.zhihu.com/people/zhihu_name">
                    <span class="fa-stack fa-2x icon-link">
                        <i class="fas fa-circle fa-stack-2x"></i>
                        <i class="fab fa-zhihu fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                
            </li>
            
        </ul>
    </div>
</div>

                </div>
                <p class="copyright text-muted">
                    Copyright &copy; 小新xx
                    <br>
                    感谢支持！
                    <br>
                    <a href="http://cs.scu.edu.cn/">海纳百川</a>, 
                    <a href="http://www.tju.edu.cn/">实事求是</a>.
                </p>
            </div>
        </div>
    </div>
</footer>

    
    
        <script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.slim.min.js"></script>
        <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
        <script src="https://cdn.bootcss.com/font-awesome/5.9.0/js/all.min.js"></script>
         
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                    CommonHTML: { linebreaks: { automatic: true } },
                    "HTML-CSS": { linebreaks: { automatic: true } },
                    SVG: { linebreaks: { automatic: true } }
                });
            </script>
            <script src='https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
        
    


<script src="/js/av.min.js"></script>
<script src="/js/valine.min.js"></script>
<script src="/js/applause-easy.js"></script>

<script>
$(document).ready(function() {
    var a = new ApplauseEasy({
        id: 'applause-easy',
        appId: "xxxxxxxxxx",
        appKey: "xxxxxxxxxx",
        img_src: "http://img.hanlindong.com/blog/site/clap.png",
        img_width: "50px",
        img_height: "50px"
    })
})
</script>


<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?e8a203f609d8ced2c71c3eee859b6941";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>




    
</body>
</html>