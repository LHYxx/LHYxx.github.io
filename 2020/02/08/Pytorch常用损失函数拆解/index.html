<!DOCTYPE html>
<html lang="zh-cmn-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content>
    <link rel="shortcut icon" href="/img/book.png">
    <title>Pytorch常用损失函数拆解-小新xx</title>
    
        
            <link href="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet">
            <link href="https://cdn.bootcss.com/academicons/1.8.6/css/academicons.min.css" rel="stylesheet">
            <link href="https://cdn.bootcss.com/font-awesome/5.9.0/css/all.min.css" rel="stylesheet">
            <link href="https://cdn.bootcss.com/animate.css/3.7.2/animate.min.css" rel="stylesheet">
        
    
    <link rel="stylesheet" href="/css/adagio.css">

    <style type="text/css">
        .jumbotron{
            background: url();
            background-repeat: no-repeat;
            background-size: 100% 100%;
        }
        .tag-list{
            font-weight: bold;
            font-size: 200;
            color: red;
        }
    </style>

<!-- 下面是对mathjax的设置 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      linebreaks: {automatic: ['\\']}
    },
    "HTML-CSS": { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true } }
  });
</script>



</head>
<body>
    <div class="container-fluid">
    <nav class="nav">
        <div class="collapse navbar-collapse" id="navbar-sm">
            
            
            <div class="navbar-nav">
                <a href="/index.html" class="nav-item nav-link">主页</a>
            </div>
            
            <div class="navbar-nav">
                <a href="/others/aboutme.html" class="nav-item nav-link">LHYxx</a>
            </div>
            
        </div>
    </nav>
</div>

<div class="d-flex d-md-none" style="width: 100%; background-color: #e9ecef">
    <div class="nav">
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-sm"
            aria-expanded="false" aria-label="Toggle Navigation">
            <i class="fas fa-bars fa-lg"></i>
        </button>
    </div>
    <nav class="navbar ml-auto">
        <a class="navbar-brand" href="/">
            
            LHYxx
            
        </a>
    </nav>
</div>


<div class="container d-none d-md-block my-navbar">
    <nav class="navbar navbar-expand-sm navbar-light bg-transparent">
        <a class="navbar-brand " href="/">
            
            LHYxx
            
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
            aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
                
                <li class="nav-item pl-2 pr-2 ">
                    <a class="nav-link" href="/index.html">主页</a>
                </li>
                
                <li class="nav-item pl-2 pr-2 ">
                    <a class="nav-link" href="/others/aboutme.html">LHYxx</a>
                </li>
                
            </ul>
        </div>
    </nav>
</div>




    <div class="jumbotron jumbotron-fluid">
<!-- <div class="jumbotron jumbotron-fluid" style="background-image: url()"> -->
    <div class="container" >
        
        <h1 class="mt-4 article-title page-title" style="font-weight: bold; color: black">Pytorch常用损失函数拆解</h1>
        
        <p class="lead text-gray mt-3" style="color: black">By 小新; Published on 2020-02-08</p>
        
        <div class="tags">
            <ul class="tag-list">
                
                <li class="tag-list-item">
                    <a class="tag-list-link" href="/tags/Pytorch/">Pytorch</a>
                </li>
                
                <li class="tag-list-item">
                    <a class="tag-list-link" href="/tags/损失函数/">损失函数</a>
                </li>
                
            </ul>
        </div>
        
    </div>
</div>
    <div class="container">
        <div class="row">
            <div class="col-md-9 pt-2">
                <div class="row">
                    <div class="col-12">
                        <main>
                            <article class="article-text page-content"><h1 id="Pytorch常用损失函数拆解"><a href="#Pytorch常用损失函数拆解" class="headerlink" title="Pytorch常用损失函数拆解"></a>Pytorch常用损失函数拆解</h1><p>本文从理论和实践两方面来全面梳理一下常用的损失函数。（避免自己总是一瓶子不满半瓶子晃荡……）。要么理论满分，编码时不会用；要么编码是会调包，但是不明白其中的计算原理。本文来科普一下。<br>我们将每个损失函数分别从理论和pytorch中的实现两个方面来拆解一下。</p>
<p>另外，解释一下torch.nn.Module 和 torch.nn.functional(俗称F)中损失函数的区别。<br>Module的损失函数例如CrossEntropyLoss、NLLLoss等是封装之后的损失函数类，是一个类，因此其中的变量可以自动维护。经常是对F中的函数的封装。<br>而F中的损失函数只是单纯的函数。<br>当然我们也可以自己构造自己的损失函数对象。有时候损失函数并不需要太复杂，没有必要特意封装一个类，直接调用F中的函数也是可以的。使用哪种看具体实现需求而定。</p>
<h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><p>交叉熵损失，是分类任务中最常用的一个损失函数。</p>
<h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><p>直接上理论公式 ：<br>$$<br>CE(\hat{x}, x)=-\sum_{i=1}^{n} x\log (\hat{x})<br>$$<br>其中$x$是真实标签，$\hat{x}$是预测值，是one-hot形式（logits），这里假设的是已经做过softmax操作了，也就是$x$与$\hat{x}$中的元素分别表示对应类别的概率。<br>举个例子，清晰明了</p>
<blockquote>
<p>x=[0, 1, 0]<br>$\hat{x}$=[0.1, 0.5, 0.4]<br>$CE(\hat{x}, x) = -0 \times log(0.1) - 1 \times log(0.5) - 0 \times log(0.4) = log(0.5)$</p>
</blockquote>
<h3 id="pytorch-实现"><a href="#pytorch-实现" class="headerlink" title="pytorch 实现"></a>pytorch 实现</h3><figure class="highlight plain"><figcaption><span>torch.nn import CrossEntropyLoss```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">举例</span><br><span class="line">当我们有了标签$x=[0, 0, 1]$与模型的输出 $logits=[0.1324, -0.1854, 0.8753]$时。模型的原始输出通常是上面的logits形式。</span><br><span class="line">然后就将他们扔到CrossEntropyLoss函数中</span><br></pre></td></tr></table></figure>

<p>loss = CrossEntropyLoss(logits, x)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们看CrossEntropyLoss函数里面的实现，是下面这样子的：</span><br></pre></td></tr></table></figure>

<pre><code>def forward(self, input, target):
    return F.cross_entropy(input, target, weight=self.weight,
                           ignore_index=self.ignore_index, reduction=self.reduction)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">是调用的torch.nn.functional（俗称F）中的cross_entropy()函数。</span><br><span class="line">参数</span><br><span class="line">+ input：预测值，（batch，dim），这里dim就是要分类的总类别数</span><br><span class="line">+ target：真实值，（batch），这里为啥是1维的？因为真实值并不是用one-hot形式表示，而是直接传类别id。</span><br><span class="line">+ weight：指定权重，（dim），可选参数，可以给每个类指定一个权重。通常在训练数据中不同类别的样本数量差别较大时，可以使用权重来平衡。</span><br><span class="line">+ ignore_index：指定忽略一个真实值，（int），也就是手动忽略一个真实值。</span><br><span class="line">+ reduction：在[none, mean, sum]中选，string型。none表示不降维，返回和target相同形状；mean表示对一个batch的损失求均值；sum表示对一个batch的损失求和。</span><br><span class="line"></span><br><span class="line">其中参数weight、ignore_index、reduction要在实例化CrossEntropyLoss对象时指定,例如：</span><br><span class="line">```loss = torch.nn.CrossEntropyLoss(reduction=&apos;none&apos;)</span><br></pre></td></tr></table></figure>

<p>我们在看一下F中的cross_entropy的实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return nll_loss(log_softmax(input, dim=1), target, weight, None, ignore_index, None, reduction)</span><br></pre></td></tr></table></figure>

<p>解释一下，先调用log_softmax,在调用null_loss。<br>log_softmax就是先softmax再取log：<br>$$<br>log_softmax(x) = \log{softmax(x)}<br>$$</p>
<p>null_loss是The negative log likelihood loss：<br>因为标签x是one-hot形式，只有一个为1，其余均为0，因此<br>$-\Sigma{x \log{\hat{x}}}$相当于取$\hat{x}$中真实值对应的值，如下：</p>
<p>$$<br>null_loss(\hat{x}, class) = -\hat{x}[class]<br>$$</p>
<p>例如假设$\hat{x}=[1, 2, 3], class=2$，则$null_loss(\hat{x}, class)=-\hat{x}[class]=-\hat{x}[2]=-2$</p>
<p>源码中给了个用法例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># input is of size N x C = 3 x 5</span><br><span class="line">input = torch.randn(3, 5, requires_grad=True)</span><br><span class="line"># each element in target has to have 0 &lt;= value &lt; C</span><br><span class="line">target = torch.tensor([1, 0, 4])</span><br><span class="line">output = F.nll_loss(F.log_softmax(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure>

<p>因此，其实CrossEntropyLoss损失，简单来说就是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CrossEntropyLoss(input, target) = null_loss(log_softmax(input, dim=1), target)</span><br></pre></td></tr></table></figure>

<p>CrossEntropyLoss中的target必须是LongTensor类型。<br>实验如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([[2, 1], [1, 2]])</span><br><span class="line">target = torch.LongTensor([1, 0])</span><br><span class="line"></span><br><span class="line">loss_fun = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">loss = loss_fun(pred, target)  </span><br><span class="line">print(loss)  # 输出为tensor(1.3133)</span><br><span class="line">loss2 = F.nll_loss(F.log_softmax(pred, dim=1), target)</span><br><span class="line">print(loss2)  # 输出为tensor(1.3133)</span><br></pre></td></tr></table></figure>

<p>数学形式就是<br>$$<br>CE(input, target) = -\log{\frac{exp(input[target])}{\Sigma_j{exp(input[j])}}}<br>$$</p>
<h2 id="torch-nn-L1Loss"><a href="#torch-nn-L1Loss" class="headerlink" title="torch.nn.L1Loss"></a>torch.nn.L1Loss</h2><h3 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h3><p>L1损失很简单，公式如下：<br>$$<br>loss(x, y)=\frac{1}{n}\Sigma_{i} {|x_{i}-y_{i}|}<br>$$<br>x是预测值，y是真实值。</p>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plain"><figcaption><span>y)```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x形状：任意形状</span><br><span class="line">y形状：与输入形状相同</span><br></pre></td></tr></table></figure>

<pre><code>pred = torch.FloatTensor([[3, 1], [1, 0]])
target = torch.FloatTensor([[1, 0], [1, 0]])
loss_fun = nn.L1Loss()
loss = loss_fun(pred, target)
print(loss)  # tensor(0.7500)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其中L1Loss的内部实现为：</span><br></pre></td></tr></table></figure>

<pre><code>def forward(self, input, target):
    return F.l1_loss(input, target, reduction=self.reduction)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">我们可以看到，其实还是对F.l1_loss的封装。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## torch.nn.MSELoss</span><br><span class="line">### 理论</span><br><span class="line">L1Loss可以理解为向量的1-范数，MSE均方误差就可以理解为向量的2-范数，或矩阵的F-范数。</span><br><span class="line">$$</span><br><span class="line">loss(x, y) = \frac&#123;1&#125;&#123;n&#125; \Sigma(x_&#123;i&#125; - y_&#123;i&#125;)^&#123;2&#125;</span><br><span class="line">$$</span><br><span class="line">x是预测值，y是真实值。</span><br><span class="line"></span><br><span class="line">### 实践</span><br><span class="line">```torch.nn.MSELoss(x, y)</span><br></pre></td></tr></table></figure>

<p>x任意形状，y与x形状相同。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([[3, 1], [1, 0]])</span><br><span class="line">target = torch.FloatTensor([[1, 0], [1, 0]])</span><br><span class="line">loss_fun = nn.MSELoss()</span><br><span class="line">loss = loss_fun(pred, target)</span><br><span class="line">print(loss)  # tensor(1.2500)</span><br></pre></td></tr></table></figure>

<p>其中MSELoss内部实现为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, input, target):</span><br><span class="line">    return F.mse_loss(input, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure>

<p>本质上是对F中mse_loss函数的封装。</p>
<h2 id="torch-nn-NLLLoss"><a href="#torch-nn-NLLLoss" class="headerlink" title="torch.nn.NLLLoss"></a>torch.nn.NLLLoss</h2><h3 id="理论-2"><a href="#理论-2" class="headerlink" title="理论"></a>理论</h3><p>NLLLoss（Negative Log Likelihood Loss），功能与CrossEntropyLoss中介绍的F.nll_loss类似：<br>$$<br>loss(x, y) = -log(x[y])<br>$$<br>和CrossEntropyLoss非常类似，实际上，先把x进行softmax，在进行log，再输入该函数中就是CrossEntropyLoss。</p>
<h3 id="实践-1"><a href="#实践-1" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plain"><figcaption><span>y)```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x是预测值，形状为（batch，dim）</span><br><span class="line">y是真实值，形状为（batch）</span><br><span class="line">形状要求与CrossEntropyLoss相同。</span><br></pre></td></tr></table></figure>

<pre><code>pred = torch.FloatTensor([[3, 1], [2, 4]])
target = torch.LongTensor([0, 1])  #target必须是Long型
loss_fun = nn.NLLLoss()
loss = loss_fun(pred, target)
print(loss)  # tensor(-3.5000)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其内部实现实际上就是调用了F.nll_loss()：</span><br></pre></td></tr></table></figure>

<pre><code>def forward(self, input, target):
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## torch.nn.KLDivLoss</span><br><span class="line">### 理论</span><br><span class="line">KL散度通常用来衡量两个连续分布之间的距离。两个分布越相似，KL散度越接近0。</span><br><span class="line">KL散度又叫相对熵，具体理论可以参考[之前的信息论基础一文](https://lhyxx.top/2019/09/15/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E7%86%B5/)</span><br><span class="line">$$</span><br><span class="line">loss(x, y) = \frac&#123;1&#125;&#123;n&#125;\Sigma_&#123;i&#125;(&#123;x_&#123;i&#125;\times\log&#123;\frac&#123;x_&#123;i&#125;&#125;&#123;y_&#123;i&#125;&#125;&#125;&#125;)</span><br><span class="line">$$</span><br><span class="line">注意，这里x与y都是分布。</span><br><span class="line">例如，$x=[0.1, 0.2, 0.7], y=[0.5, 0.2, 0.3]$</span><br><span class="line">则$loss(x, y) = \frac&#123;1&#125;&#123;3&#125;\times(0.1\times\log&#123;\frac&#123;0.1&#125;&#123;0.5&#125;&#125; + 0.2\times\log&#123;\frac&#123;0.2&#125;&#123;0.2&#125;&#125; + 0.7\times\log&#123;\frac&#123;0.7&#125;&#123;0.3&#125;&#125;)=0.43216$</span><br><span class="line">本例中计算的log都是以e为底的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 实践</span><br><span class="line">```torch.nn.KLDivLoss(x, y)</span><br></pre></td></tr></table></figure>

<p>试验测试torch.nn.KLDivLoss，计算KL(pred|target)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([0.1, 0.2, 0.7])</span><br><span class="line">target = torch.FloatTensor([0.5, 0.2, 0.3])</span><br><span class="line">loss_fun = nn.KLDivLoss(reduction=&apos;sum&apos;)  # reduction可选 none, sum, mean, batchmean</span><br><span class="line">loss = loss_fun(target.log(), pred)</span><br><span class="line">print(loss)  # tensor(0.4322)</span><br><span class="line"></span><br><span class="line">#上面的计算过程等价于下面</span><br><span class="line">a = (0.1 * np.log(1/5) + 0.2 * np.log(1) + 0.7 * np.log(7/3))</span><br><span class="line">print(a)  # 0.43216</span><br></pre></td></tr></table></figure>

<p>注意，使用nn.KLDivLoss计算KL(pred|target)时，需要将pred和target调换位置，而且target需要先取对数：<figure class="highlight plain"><figcaption><span>pred)```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">也就是说，pytorch中的KLDivLoss计算方法为：</span><br><span class="line">$$</span><br><span class="line">loss(x, y) = \frac&#123;1&#125;&#123;n&#125;\Sigma_&#123;i&#125;(&#123;y_&#123;i&#125;\times\log&#123;\frac&#123;y_&#123;i&#125;&#125;&#123;x_&#123;i&#125;&#125;&#125;&#125;)</span><br><span class="line">$$</span><br><span class="line">并且输入x应该是去log之后的概率。</span><br><span class="line"></span><br><span class="line">## torch.nn.BCELoss</span><br><span class="line">### 理论</span><br><span class="line">二分类交叉熵损失。上公式：</span><br><span class="line">$$</span><br><span class="line">loss(x, y) = -\frac&#123;1&#125;&#123;n&#125; \Sigma_&#123;i&#125;(&#123;x_&#123;i&#125;\times log(y_&#123;i&#125;)+(1-x_&#123;i&#125;)\times log(1-y_&#123;i&#125;))&#125;</span><br><span class="line">$$</span><br><span class="line">和CrossEntropyLoss损失类似，CrossEntropyLoss用于多分类，如果类别只有2类，就变成了二分类任务，那么交叉熵损失公式就变成了上面这个二分类交叉熵公式。</span><br><span class="line"></span><br><span class="line">### 实践</span><br><span class="line">```torch.nn.BCELoss(x, y)</span><br></pre></td></tr></table></figure></p>
<p>x形状（batch，*），y形状与x相同。<br>x与y中每个元素，表示的是该维度上属于（或不属于）这个类的概率，所以第二维的数值表示的是一个batch里需要预测的对象的个数。<br>因为是二分类，非0即1，因此对于每个对象，只需要1位就可以表示，即只预测属于的概率或者只预测不属于的概率。<br>另外，pytorch中的BCELoss可以为每个类指定权重。通常，当训练数据中正例和反例的比例差别较大时，可以为其赋予不同的权重，weight的形状应该是一个一维的，元素的个数等于类别数。</p>
<p>实际使用如下例，计算BCELoss(pred, target)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pred = torch.FloatTensor([0.4, 0.1])</span><br><span class="line">target = torch.FloatTensor([0.2, 0.8])</span><br><span class="line">loss_fun = nn.BCELoss(reduction=&apos;mean&apos;)  # reduction可选 none, sum, mean, batchmean</span><br><span class="line">loss = loss_fun(pred, target)</span><br><span class="line">print(loss)  # tensor(1.2275)</span><br><span class="line"></span><br><span class="line">a = -(0.2 * np.log(0.4) + 0.8 * np.log(0.6) + 0.8 * np.log(0.1) + 0.2 * np.log(0.9))/2</span><br><span class="line">print(a)  # 1.2275294114572126</span><br><span class="line"></span><br><span class="line"># 按照理论中的公式手算结果</span><br><span class="line">b = -(0.4 * np.log(0.2) + 0.6 * np.log(0.8) + 0.1 * np.log(0.8) + 0.9 * np.log(0.2)) / 2</span><br><span class="line">print(b)  # 1.1242348860421387</span><br></pre></td></tr></table></figure>

<p>可以看到，计算BCELoss(pred, target)与上面理论中的公式不一样，也就是说，pytorch中的BCELoss计算方法为：<br>$$<br>loss(x, y) = -\frac{1}{n} \Sigma_{i}({y_{i}\times log(x_{i})+(1-y_{i})\times log(1-x_{i}))}<br>$$</p>
<h2 id="torch-nn-BCEWithLogitsLoss"><a href="#torch-nn-BCEWithLogitsLoss" class="headerlink" title="torch.nn.BCEWithLogitsLoss"></a>torch.nn.BCEWithLogitsLoss</h2><h3 id="理论-3"><a href="#理论-3" class="headerlink" title="理论"></a>理论</h3><p>该函数实际上与BCELoss相同，只是BCELoss的输入x，在输入之前需要先经过sigmoid激活函数映射到（0， 1）区间，而该函数将sigmoid与BCELoss整合到一起了。把sigmoid和bce的过程整合到一起。</p>
<h3 id="实践-2"><a href="#实践-2" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plain"><figcaption><span>y)```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x与y的形状要求与BCELoss相同。</span><br></pre></td></tr></table></figure>

<pre><code>pred = torch.FloatTensor([0.4, 0.1])
target = torch.FloatTensor([0.2, 0.8])
loss_fun = nn.BCEWithLogitsLoss(reduction=&apos;mean&apos;)  # reduction可选 none, sum, mean, batchmean
loss = loss_fun(pred, target)
print(loss)  # tensor(0.7487)

# 上面的过程与下面的过程结果相同
loss_fun = nn.BCELoss(reduction=&apos;mean&apos;)  # reduction可选 none, sum, mean, batchmean
loss = loss_fun(torch.sigmoid(pred), target)
print(loss)  # tensor(0.7487)</code></pre><p>```<br>可以看出，先对输入pred调用sigmoid，在调用BCELoss，结果就等于直接调用BCEWithLogitsLoss。</p>
</article>
                        </main>
                        
                        
                    </div>
                </div>
                <div class="row mt-5 mb-5">
                    <div class="col-12">
                        <div class="row">
    <div class="col">
        <nav aria-label="paginator" class="paginator">
            <ul class="pagination d-none d-md-flex pagination-lg justify-content-center">
                <li class="page-item  disabled ">
                    <a class="page-link"
                        href=""
                        aria-label="Previous">
                        <span aria-hidden="true">&laquo;
                            No more</span>
                        <span class="sr-only">Previous</span>
                    </a>
                </li>
                <li class="page-item ">
                    <a class="page-link"
                        href="/2019/11/30/配置cuda和pytorch-gpu/"
                        aria-label="Next">
                        <span
                            aria-hidden="true">配置cuda和pytorch-gpu
                            &raquo;</span>
                        <span class="sr-only">Next</span>
                    </a>
                </li>
            </ul>
            <ul class="pagination d-md-none justify-content-center">
                <li class="page-item  disabled ">
                    <a class="page-link"
                        href=""
                        aria-label="Previous">
                        <span aria-hidden="true">&laquo;
                            No more</span>
                        <span class="sr-only">Previous</span>
                    </a>
                </li>
                <li class="page-item ">
                    <a class="page-link"
                        href="/2019/11/30/配置cuda和pytorch-gpu/"
                        aria-label="Next">
                        <span
                            aria-hidden="true">配置cuda和pytorch-gpu
                            &raquo;</span>
                        <span class="sr-only">Next</span>
                    </a>
                </li>
            </ul>
        </nav>
    </div>
</div>



                    </div>
                </div>
                <div class="row">
                    <div class="col-12">
                        <div id="vcomment"></div>
                    </div>
                </div>
            </div>
            <div class="col-md-3">
                <div class="container pt-4 page-sidebar">
                    
                    <div class="row">
    <div class="col">
        <h6>APPLAUSE FOR ME</h6>
        <div id="applause-easy"></div>
    </div>
</div>
                    
                    <hr class="row">
                    <div class="row toc-container">
                        <div class="col-12">
                            <h6>NAVIGATION</h6>
                            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch常用损失函数拆解"><span class="toc-text">Pytorch常用损失函数拆解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CrossEntropyLoss"><span class="toc-text">CrossEntropyLoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch-实现"><span class="toc-text">pytorch 实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-L1Loss"><span class="toc-text">torch.nn.L1Loss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论-1"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实践"><span class="toc-text">实践</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-NLLLoss"><span class="toc-text">torch.nn.NLLLoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论-2"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实践-1"><span class="toc-text">实践</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-BCEWithLogitsLoss"><span class="toc-text">torch.nn.BCEWithLogitsLoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理论-3"><span class="toc-text">理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实践-2"><span class="toc-text">实践</span></a></li></ol></li></ol></li></ol>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <footer>
    <div class="jumbotron jumbotron-fluid mb-0">
        <div class="container-fluid">
            <div class="col text-center">
                <div class="bottom-social">
                    <div class="row">
    <div class="col text-center">
        <ul class="list-inline">
            
            <li class="list-inline-item">
                
                <a href="https://github.com/LHYxx">
                    <span class="fa-stack fa-2x icon-link">
                        <i class="fas fa-circle fa-stack-2x"></i>
                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                
            </li>
            
            <li class="list-inline-item">
                
                <a href="https://www.zhihu.com/people/zhihu_name">
                    <span class="fa-stack fa-2x icon-link">
                        <i class="fas fa-circle fa-stack-2x"></i>
                        <i class="fab fa-zhihu fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                
            </li>
            
        </ul>
    </div>
</div>

                </div>
                <p class="copyright text-muted">
                    Copyright &copy; 小新xx
                    <br>
                    感谢支持！
                    <br>
                    <a href="http://cs.scu.edu.cn/">海纳百川</a>, 
                    <a href="http://www.tju.edu.cn/">实事求是</a>.
                </p>
            </div>
        </div>
    </div>
</footer>

    
    
        <script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.slim.min.js"></script>
        <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
        <script src="https://cdn.bootcss.com/font-awesome/5.9.0/js/all.min.js"></script>
         
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                    CommonHTML: { linebreaks: { automatic: true } },
                    "HTML-CSS": { linebreaks: { automatic: true } },
                    SVG: { linebreaks: { automatic: true } }
                });
            </script>
            <script src='https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
        
    


<script src="/js/av.min.js"></script>
<script src="/js/valine.min.js"></script>
<script src="/js/applause-easy.js"></script>

<script>
$(document).ready(function() {
    var a = new ApplauseEasy({
        id: 'applause-easy',
        appId: "xxxxxxxxxx",
        appKey: "xxxxxxxxxx",
        img_src: "http://img.hanlindong.com/blog/site/clap.png",
        img_width: "50px",
        img_height: "50px"
    })
})
</script>


<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?e8a203f609d8ced2c71c3eee859b6941";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>




    
</body>
</html>