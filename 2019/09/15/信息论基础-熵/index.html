<!DOCTYPE html>
<html lang="zh-cmn-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content>
    <link rel="shortcut icon" href="/img/book.png">
    <title>信息论基础-信息熵、条件熵、相对熵、交叉熵-小新xx</title>
    
        
            <link href="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet">
            <link href="https://cdn.bootcss.com/academicons/1.8.6/css/academicons.min.css" rel="stylesheet">
            <link href="https://cdn.bootcss.com/font-awesome/5.9.0/css/all.min.css" rel="stylesheet">
            <link href="https://cdn.bootcss.com/animate.css/3.7.2/animate.min.css" rel="stylesheet">
        
    
    <link rel="stylesheet" href="/css/adagio.css">

<!-- 下面是对mathjax的设置 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      linebreaks: {automatic: ['\\']}
    },
    "HTML-CSS": { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true } }
  });
</script>



</head>
<body>
    <div class="container-fluid">
    <nav class="nav">
        <div class="collapse navbar-collapse" id="navbar-sm">
            
            
            <div class="navbar-nav">
                <a href="/index.html" class="nav-item nav-link">主页</a>
            </div>
            
            <div class="navbar-nav">
                <a href="/" class="nav-item nav-link">LHYxx</a>
            </div>
            
        </div>
    </nav>
</div>

<div class="d-flex d-md-none" style="width: 100%; background-color: #e9ecef">
    <div class="nav">
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-sm"
            aria-expanded="false" aria-label="Toggle Navigation">
            <i class="fas fa-bars fa-lg"></i>
        </button>
    </div>
    <nav class="navbar ml-auto">
        <a class="navbar-brand" href="/">
            
            LHYxx
            
        </a>
    </nav>
</div>


<div class="container d-none d-md-block my-navbar">
    <nav class="navbar navbar-expand-sm navbar-light bg-transparent">
        <a class="navbar-brand " href="/">
            
            LHYxx
            
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
            aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
                
                <li class="nav-item pl-2 pr-2 ">
                    <a class="nav-link" href="/index.html">主页</a>
                </li>
                
                <li class="nav-item pl-2 pr-2 ">
                    <a class="nav-link" href="/">LHYxx</a>
                </li>
                
            </ul>
        </div>
    </nav>
</div>




    <div class="jumbotron jumbotron-fluid"">
    <div class="container">
        
        <h1 class="mt-4 article-title page-title">信息论基础-信息熵、条件熵、相对熵、交叉熵</h1>
        
        <p class="lead text-gray mt-3">By 小新; Published on 2019-09-15</p>
        
        <div class="tags">
            <ul class="tag-list">
                
                <li class="tag-list-item">
                    <a class="tag-list-link" href="/tags/信息论/">信息论</a>
                </li>
                
            </ul>
        </div>
        
    </div>
</div>
    <div class="container">
        <div class="row">
            <div class="col-md-9 pt-2">
                <div class="row">
                    <div class="col-12">
                        <main>
                            <article class="article-text page-content"><p>1948年，克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以也被称为香农熵 (Shannon entropy)，信息熵 (information entropy)。<br>我们有时需要定量的来度量信息量的大小，因此提出了<strong>信息熵</strong>，用来度量包含的信息量的多少。</p>
<h1 id="自信息熵（information-entropy）"><a href="#自信息熵（information-entropy）" class="headerlink" title="自信息熵（information entropy）"></a>自信息熵（information entropy）</h1><p>考虑一个随机变量x，它的分布概率为p(x)。我们令$$I(x)=-log(p(x))$$ 叫做随机变量x的自信息量（self-information）。图像如下：<br><img src="self-information.png" alt="自信息熵" title="自信息熵"><br><strong>自信息熵描述的是随机变量某个事件发生所带来的信息量</strong>，举个例子，假如对x进行分类，x属于 $X_0$ 或者 $X_1$ 两个类别的概率如下：</p>
<table>
<thead>
<tr>
<th>x</th>
<th>$X_0$</th>
<th>$X_1$</th>
</tr>
</thead>
<tbody><tr>
<td>p(x)</td>
<td>0.5</td>
<td>0.5</td>
</tr>
</tbody></table>
<p>也就是x属于两个类别的概率相等，都是0.5。那么变量x属于$X_0$和属于$X_1$的自信息量分别为：<br>$$I(X_0)=-log(0.5)=1$$<br>$$I(X_1)=-log(0.5)=1$$<br>也就是说，变量x属于两个类别所带来的信息量都是1。</p>
<p>如果x属于$X_0$的概率大于属于$X_1$的概率：</p>
<table>
<thead>
<tr>
<th>x</th>
<th>$X_0$</th>
<th>$X_1$</th>
</tr>
</thead>
<tbody><tr>
<td>p(x)</td>
<td>0.9</td>
<td>0.1</td>
</tr>
</tbody></table>
<p>则自信息量为：<br>$$I(X_0)=-log(0.9)=0.152003$$<br>$$I(X_1)=-log(0.1)=3.321928$$<br>也就是说，x属于$X_0$带来的信息量小于x属于$X_1$带来的信息量。<br>我们从直观上来理解一下，因为x属于$X_0$的概率非常大（0.9），所以说x属于$X_0$可以说是比较意料之中的事，因为我们随便猜也会猜属于$X_0$，所以说x属于$X_0$带来的信息量比较小。<br>相反，x属于$X_1$的概率非常小(0.1)，当概率P越小，x出现的概率就越小，一旦出现所获得的信息量就越大。所以从自信息的图上来看，p(x)越小，自信息量越大。<br>通常称I(x)为消息x的<strong>自信息</strong>，它具有随机变量的性质，但自信息量不能表示信源总体的不确定度。</p>
<h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>如果x所有可能的取值（对分类问题就是所有可能的类别）为X={$X_1$, $X_2$, … , $X_n$}，其概率分布为$P(X=x_i)=P_i(i=1,2,…,n)$，则随机变量x的信息熵定义为：<br>$$<br>H(X)=-\sum_{x} p(x) \log p(x)=-\sum_{i=1}^{n} p\left(x_{i}\right) \log p\left(x_{i}\right)<br>$$<br>根据公式，信息熵可以理解为自信息量的数学期望。自信息量表示某个事件的信息量，通过乘以该事件发生的概率并和，也就是所有可能发生事件的信息量的期望。<br>信息熵的物理意义可以代表一个事件的不确定性。例如，一个确定事件的信息熵应该最小<em>（平均信息量最小）</em>，不确定事件的信息熵应该最大<em>（平均信息量最大）</em>。熵越大，就越无序，越混乱，越不确定。</p>
<h2 id="例1，确定事件"><a href="#例1，确定事件" class="headerlink" title="例1，确定事件"></a>例1，确定事件</h2><p>例如，随机变量x概率分布如下：</p>
<table>
<thead>
<tr>
<th>x</th>
<th>$X_0$</th>
<th>$X_1$</th>
</tr>
</thead>
<tbody><tr>
<td>p(x)</td>
<td>1</td>
<td>0</td>
</tr>
</tbody></table>
<p>首先算自信息量为：<br>$I(X_0)=-\log(1)=0$<br>$I(X_1)=-\log(0)=Infiniti$<br>信息熵为：<br>$$<br>\begin{aligned}<br>H(X)&amp;=-\sum_{x} p(x) \log p(x) \\<br>&amp;=p(X_0) \times I(X_0)+p(X_1) \times I(X_1) \\<br>&amp;=p(X_0) \times \log(p(X_0)) + p(X_1) \times \log(p(X_1)) \\<br>&amp;=1 \times 0+0 \times Infiniti=0<br>\end{aligned}<br>$$<br>上面的例子实际是一个确定事件，x必定属于$X_0$，因此对于确定事件的信息熵最小，也表示该事件的不确定性最小。</p>
<h2 id="例2，等概率事件"><a href="#例2，等概率事件" class="headerlink" title="例2，等概率事件"></a>例2，等概率事件</h2><p>如果x的概率分布不是确定事件，而是服从如下的概率分布：</p>
<table>
<thead>
<tr>
<th>x</th>
<th>$X_0$</th>
<th>$X_1$</th>
</tr>
</thead>
<tbody><tr>
<td>p(x)</td>
<td>0.5</td>
<td>0.5</td>
</tr>
</tbody></table>
<p>则信息熵计算为：<br>$$H(X)=-\sum_{x} p(x) \log p(x) \\<br>=p(X_0) \times I(X_0)+p(X_1) \times I(X_1) \\<br>=p(X_0) \times \log(p(X_0)) + p(X_1) \times \log(p(X_1)) \\<br>=0.5 \times 1+0.5 \times 1=1$$<br>由于x属于两个类别的概率相等，都是0.5，因此x是一个高度随机事件，具有高度的不确定性，信息熵为1。</p>
<h2 id="例3，随机事件"><a href="#例3，随机事件" class="headerlink" title="例3，随机事件"></a>例3，随机事件</h2><p>如果x概率分布如下：</p>
<table>
<thead>
<tr>
<th>x</th>
<th>$X_0$</th>
<th>$X_1$</th>
</tr>
</thead>
<tbody><tr>
<td>p(x)</td>
<td>0.9</td>
<td>0.1</td>
</tr>
</tbody></table>
<p>则信息熵计算为：<br>$$H(X)=-\sum_{x} p(x) \log p(x) \\<br>=p(X_0) \times I(X_0)+p(X_1) \times I(X_1) \\<br>=p(X_0) \times \log(p(X_0)) + p(X_1) \times \log(p(X_1)) \\<br>=0.9 \times 0.15200+0.1 \times 3.321928=0.4689928$$</p>
<p>该x是随机事件，但是随机性较小，所以信息熵较小。<br>可以得出结论，当x的所有类别的概率相等时，x的不确定性最大，信息熵最大。当x为确定事件时，x的不确定性最小，信息熵最小（=0）。</p>
<h2 id="另一种理解方式"><a href="#另一种理解方式" class="headerlink" title="另一种理解方式"></a>另一种理解方式</h2><p>假设我们要对信息进行传递，需要对信息进行编码，信息熵表示的就是我们对这些信息进行最优编码所需要的bit数。<br>在香农信息论中，用基于P的编码方法去编码来自P的样本，其最优编码平均所需要的比特个数（即这个字符集的熵）为：<br><img src="%E4%BF%A1%E6%81%AF%E7%86%B5%E8%A7%A3%E9%87%8A1.jpg" alt="信息熵" title="信息熵"><br>P(x)表示x在信源中出现的频率，例如在文本中则可以表示字符x在整个语料库中出现的频率。$\log (\frac{1}{P(x)})$表示在该信源中编码x所需要的长度。在最优编码中，为了使所有信息的平均编码长度最小，通常会为出现频率较大的字符赋予较短的编码长度（回想Huffman编码）。所以上面的式子表示的就是一个数据集中最优编码平均所需要的比特个数（即这个字符集的熵），如果是计算机中2进制编码，则式子中的log以2为底，以此类推。</p>
<p>例子1中，确定事件，所以不需要编码，需要0bit。<br>例子2中，等概率事件，使用1bit就可以编码。<br>例子3中，随机事件，通过计算可以得知，大概需要0.469bit对信息进行编码。<br>所以，信息熵可以理解为对事件进行编码所需要的最小位数。可以反映事件的信息量。</p>
<h1 id="条件熵-Conditional-entropy"><a href="#条件熵-Conditional-entropy" class="headerlink" title="条件熵 (Conditional entropy)"></a>条件熵 (Conditional entropy)</h1><p>条件熵的原始形式：<br>$$<br>\begin{aligned} H(Y | X) &amp;=\sum_{x} p(x) H(Y | X=x) \\<br>&amp;=-\sum_{x} p(x) \sum_{y} p(y | x) \log p(y | x) \\<br>&amp;=-\sum_{x} \sum_{y} p(x, y) \log p(y | x) \\<br>&amp;=-\sum_{x, y} p(x, y) \log p(y | x) \end{aligned}<br>$$<br>条件熵表示在已知随机变量 X 的条件下，Y 的条件概率分布的熵对随机变量 X 的数学期望。<br>条件熵 H(Y|X) 表示在已知随机变量 X 的条件下随机变量 Y 的不确定性。<br>通常条件熵H(Y|X)要比Y的信息熵H(Y)小，因为条件熵H(Y|X)相当于是在知道了X的信息之后Y的不确定性。而增加了X的信息通常会减少Y的不确定性。加入了额外信息而导致总的不确定性减小，这就引出了信息增益的概念。<br>下面例子中具体体会。</p>
<p>另外，条件熵 H(Y|X) 相当于联合熵 H(X,Y) 减去单独的熵 H(X)，即<br>$$<br>H(Y | X)=H(X, Y)-H(X)<br>$$<br>证明如下：<br>$$<br>\begin{aligned} H(X, Y) &amp;=-\sum_{x, y} p(x, y) \log p(x, y) \\<br>&amp;=-\sum_{x, y} p(x, y) \log (p(y | x) p(x)) \\<br>&amp;=-\sum_{x, y} p(x, y) \log p(y | x)-\sum_{x, y} p(x, y) \log p(x) \\<br>&amp;=H(Y | X)-\sum_{x, y} p(x, y) \log p(x) \\<br>&amp;=H(Y | X)-\sum_{x}^{x} \log p(x) \sum_{y} p(x, y) \\<br>&amp;=H(Y | X)-\sum_{x}^{x}(\log p(x)) p(x) \\<br>&amp;=H(Y | X)-\sum_{x}^{x} p(x) \log p(x) \\<br>&amp;=H(Y | X)+H(X) \end{aligned}<br>$$</p>
<p>直观上理解，<br>H(Y, X) = H(X)+H(Y|X)<br>XY总体的信息量 = X的信息量 + 已知X的信息下Y的信息量<br>或<br>H(X, Y)-H(X) = H(Y|X)<br>XY总体的不确定性，由于提供了X的额外信息，减去了X的不确定性，剩下的是已知X情况下的Y的不确定性。</p>
<h2 id="条件熵例子"><a href="#条件熵例子" class="headerlink" title="条件熵例子"></a>条件熵例子</h2><table>
<thead>
<tr>
<th>Person</th>
<th>头发长度</th>
<th>体重</th>
<th>年龄</th>
<th>性别</th>
</tr>
</thead>
<tbody><tr>
<td>no1</td>
<td>0</td>
<td>250</td>
<td>36</td>
<td>男</td>
</tr>
<tr>
<td>no2</td>
<td>10</td>
<td>150</td>
<td>34</td>
<td>女</td>
</tr>
<tr>
<td>no3</td>
<td>2</td>
<td>90</td>
<td>10</td>
<td>男</td>
</tr>
<tr>
<td>no4</td>
<td>6</td>
<td>78</td>
<td>8</td>
<td>女</td>
</tr>
<tr>
<td>no5</td>
<td>4</td>
<td>20</td>
<td>1</td>
<td>女</td>
</tr>
<tr>
<td>no6</td>
<td>1</td>
<td>170</td>
<td>70</td>
<td>男</td>
</tr>
<tr>
<td>no7</td>
<td>8</td>
<td>160</td>
<td>41</td>
<td>女</td>
</tr>
<tr>
<td>no8</td>
<td>10</td>
<td>180</td>
<td>38</td>
<td>男</td>
</tr>
<tr>
<td>no9</td>
<td>6</td>
<td>200</td>
<td>45</td>
<td>男</td>
</tr>
</tbody></table>
<p>我们令X表示性别，Y表示头发长度，规定长度&gt;5为长发，长度&lt;5为短发，也就是Y表示头发长度是否大于5。<br>下面计算X与Y的联合熵、X的信息熵、Y的信息熵、Y作为条件下X的条件熵、X作为条件下Y的条件熵</p>
<p>H(X, Y) = H(男,短发)3 + H(男,长发)2 + H(女,短发)1 + H(女,长发)3<br>$$<br>H(X, Y) = 3/9 \times \log(3/9) + 2/9 \times \log(2/9) + 1/9 \times \log(1/9) + 3/9 \times \log(3/9)=1.8910611120726526\\<br>$$</p>
<p>H(Y) = H(短发)4 + H(长发)5<br>$$<br>H(Y) = 4/9 \times \log(4/9) + 5/9 \times \log(5/9)=0.9910760598382222\\<br>$$</p>
<p>H(X) = H(男)4 + H(女)5<br>$$<br>H(X) = 4/9 \times \log(4/9) + 5/9 \times \log(5/9)=0.9910760598382222\\<br>$$</p>
<p>H(Y|X) = p(男) * (H(短发|男) + H(长发|男)) + p(女) * (H(短发|女) + H(长发|女))<br>$$<br>H(Y|X) = 5/9 \times (3/5 \times \log(3/5) + 2/5 \times \log(2/5)) + 4/9 \times  (1/4 \times \log(1/4) + 3/4 \times \log(3/4))=0.8999850522344305\\<br>$$</p>
<p>H(X|Y) = p(短发) * (H(男|短发) + H(女|短发)) + p(长发) * (H(男|长发) + H(女|长发))<br>$$<br>H(X|Y) = 4/9 \times (3/4 \times log(3/4) + 1/4 \times \log(1/4)) + 5/9 \times (2/5 \times \log(2/5) + 3/5 \times \log(3/5))=0.8999850522344305\\<br>$$<br>可以验证$H(Y | X)=H(X, Y)-H(X)$成立。</p>
<h1 id="相对熵-relative-entropy"><a href="#相对熵-relative-entropy" class="headerlink" title="相对熵(relative entropy)"></a>相对熵(relative entropy)</h1><p>上面我们说，用基于P的编码方法去编码来自P的样本，其最优编码平均所需要的比特个数为：<br><img src="%E4%BF%A1%E6%81%AF%E7%86%B5%E8%A7%A3%E9%87%8A1.jpg" alt="信息熵" title="信息熵解释1"><br>用基于P的编码方法去编码来自Q的样本，则所需要的比特个数变为：<br><img src="%E4%BF%A1%E6%81%AF%E7%86%B5%E8%A7%A3%E9%87%8A2.jpg" alt="信息熵" title="信息熵解释2"><br>于是，我们即可得出P与Q的KL散度：<br>$$<br>\begin{array}{c}{\mathrm{D}(\mathrm{P} || \mathrm{Q})=H^{\prime}(x)-H(x)=\sum_{x \in X} P(x) * \log \left(\frac{1}{Q(x)}\right)-\sum_{x \in X} P(x) * \log \left(\frac{1}{P(x)}\right)} \\ {=\sum_{x \in X} P(x) *\left[\log \left(\frac{P(x)}{Q(x)}\right)\right]}\end{array}<br>$$<br>如果P和Q中数据分布是一模一样的，那么H(x) = H’(x)，所以 KL(P||Q)=0。如果P和Q分布差距越大，$D_{KL}(P||Q)$越大。因此，两个分布的相对熵又叫KL散度，常用来衡量两个分布的相似程度。<br>相对熵（KL散度）具有如下性质：</p>
<ol>
<li>如果P和Q两个分布相同，则$D_{KL}(P||Q)=0$。</li>
<li>相对熵不具有对称性：$D_{KL}(P||Q) \neq D_{KL}(Q||P)$</li>
<li>相对熵恒大于等于0：$D_{KL}(P||Q) \geq 0$。证明如下（利用Jensen不等式）：<br>$$<br>\begin{aligned} D_{K L}(p || q) &amp;=\sum_{x} p(x) \log \frac{p(x)}{q(x)} \\ &amp;=-\sum_{x} p(x) \log \frac{q(x)}{p(x)} \\ &amp;=-E_{p(x)}\left(\log \frac{q(x)}{p(x)}\right) \\ &amp; \geq-\log E_{p(x)}\left(\frac{q(x)}{p(x)}\right) \\ &amp;=-\log \sum_{x} p(x) \frac{q(x)}{p(x)} \\ &amp;=-\log \sum_{x} q(x) \end{aligned}<br>$$<br>因为$\sum_{x} p(x) = 1$，所以$D_{KL}(P||Q) \geq 0$</li>
</ol>
<p><strong>总结：相对熵可以用来衡量两个概率分布之间的差异，上面公式的意义就是求 p 与 q 之间的对数差在 p 上的期望值。</strong></p>
<h2 id="相对熵例子"><a href="#相对熵例子" class="headerlink" title="相对熵例子"></a>相对熵例子</h2><p>假如一个数据集中存在0和1两种字符，0和1的真实概率分布为A，但是我们不知道A的具体分布。现在我们想要用另外的分布去拟合分布A。<br>假设A的真实分布为：p(0)=0.5, p(1)=0.5。（但是我们并不知道这个真实分布）<br>我们通过对样本进行观察统计，得到两个分布：<br>分布B：p(0)=0.1, p(1)=0.9<br>分布C：p(0)=0.4, p(1)=0.6<br>则分布A和分布B之间的相对熵（KL散度）为：<br>$$<br>D_{KL}(A||B) = 0.5 \times \log (\frac{0.5}{0.1}) + 0.5 \times \log (\frac{0.5}{0.9}) = 0.7369655941662061<br>$$<br>A与C之间的相对熵（KL散度）为：<br>$$<br>D_{KL}(A||C) = 0.5 \times \log (\frac{0.5}{0.4}) + 0.5 \times \log (\frac{0.5}{0.6}) = 0.029446844526784283<br>$$<br>可以看出C分布更接近A分布，所以A与C之间的相对熵更小。</p>
<h1 id="交叉熵-cross-entropy"><a href="#交叉熵-cross-entropy" class="headerlink" title="交叉熵(cross entropy)"></a>交叉熵(cross entropy)</h1><p>终于到了大名鼎鼎的交叉熵了。交叉熵的名字几乎是人人皆知，机器学习中也经常使用。但是这个交叉熵到底是怎么个原理呢。<br>我们将上面的KL散度的公式稍微变形一下：<br>$$<br>\begin{aligned} D_{K L}(p || q) &amp;=\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(p\left(x_{i}\right)\right)-\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right)<br>\\ &amp;=-H(p(x))+\left[-\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right)\right] \end{aligned}<br>$$<br>等式的前一部分正好就是p的信息熵，后面那一部分，就是交叉熵：<br>$$<br>H(p, q)=-\sum_{i=1}^{n} p(x_i)\log (q(x_i))<br>$$<br>在机器学习中，我们需要评估label和predicts之间的差距，并且我们希望我们预测的prediction的分布与真实的label的分布能够尽量一致，正好可以使用KL散度，即$D_{KL} (y|| \hat{y})$。由于KL散度的前一项$-H(y)$不变（因为训练数据是固定的，所以y的分布是固定的），所以在优化过程中，只需要最优化交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://blog.csdn.net/tsyccnh/article/details/79163834" target="_blank" rel="noopener">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉</a><br><a href="https://www.cnblogs.com/kyrieng/p/8694705.html" target="_blank" rel="noopener">详解机器学习中的熵、条件熵、相对熵和交叉熵</a></p>
</article>
                        </main>
                        
                        
                    </div>
                </div>
                <div class="row mt-5 mb-5">
                    <div class="col-12">
                        <div class="row">
    <div class="col">
        <nav aria-label="paginator" class="paginator">
            <ul class="pagination d-none d-md-flex pagination-lg justify-content-center">
                <li class="page-item  disabled ">
                    <a class="page-link"
                        href=""
                        aria-label="Previous">
                        <span aria-hidden="true">&laquo;
                            No more</span>
                        <span class="sr-only">Previous</span>
                    </a>
                </li>
                <li class="page-item ">
                    <a class="page-link"
                        href="/2019/09/06/matplotlib基础教程-3/"
                        aria-label="Next">
                        <span
                            aria-hidden="true">matplotlib基础教程-3
                            &raquo;</span>
                        <span class="sr-only">Next</span>
                    </a>
                </li>
            </ul>
            <ul class="pagination d-md-none justify-content-center">
                <li class="page-item  disabled ">
                    <a class="page-link"
                        href=""
                        aria-label="Previous">
                        <span aria-hidden="true">&laquo;
                            No more</span>
                        <span class="sr-only">Previous</span>
                    </a>
                </li>
                <li class="page-item ">
                    <a class="page-link"
                        href="/2019/09/06/matplotlib基础教程-3/"
                        aria-label="Next">
                        <span
                            aria-hidden="true">matplotlib基础教程-3
                            &raquo;</span>
                        <span class="sr-only">Next</span>
                    </a>
                </li>
            </ul>
        </nav>
    </div>
</div>



                    </div>
                </div>
                <div class="row">
                    <div class="col-12">
                        <div id="vcomment"></div>
                    </div>
                </div>
            </div>
            <div class="col-md-3">
                <div class="container pt-4 page-sidebar">
                    
                    <div class="row">
    <div class="col">
        <h6>APPLAUSE FOR ME</h6>
        <div id="applause-easy"></div>
    </div>
</div>
                    
                    <hr class="row">
                    <div class="row toc-container">
                        <div class="col-12">
                            <h6>NAVIGATION</h6>
                            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#自信息熵（information-entropy）"><span class="toc-text">自信息熵（information entropy）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#信息熵"><span class="toc-text">信息熵</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#例1，确定事件"><span class="toc-text">例1，确定事件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#例2，等概率事件"><span class="toc-text">例2，等概率事件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#例3，随机事件"><span class="toc-text">例3，随机事件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#另一种理解方式"><span class="toc-text">另一种理解方式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#条件熵-Conditional-entropy"><span class="toc-text">条件熵 (Conditional entropy)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#条件熵例子"><span class="toc-text">条件熵例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#相对熵-relative-entropy"><span class="toc-text">相对熵(relative entropy)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#相对熵例子"><span class="toc-text">相对熵例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#交叉熵-cross-entropy"><span class="toc-text">交叉熵(cross entropy)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考链接"><span class="toc-text">参考链接</span></a></li></ol>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <footer>
    <div class="jumbotron jumbotron-fluid mb-0">
        <div class="container-fluid">
            <div class="col text-center">
                <div class="bottom-social">
                    <div class="row">
    <div class="col text-center">
        <ul class="list-inline">
            
            <li class="list-inline-item">
                
                <a href="https://github.com/LHYxx">
                    <span class="fa-stack fa-2x icon-link">
                        <i class="fas fa-circle fa-stack-2x"></i>
                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                
            </li>
            
            <li class="list-inline-item">
                
                <a href="https://www.zhihu.com/people/zhihu_name">
                    <span class="fa-stack fa-2x icon-link">
                        <i class="fas fa-circle fa-stack-2x"></i>
                        <i class="fab fa-zhihu fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                
            </li>
            
        </ul>
    </div>
</div>

                </div>
                <p class="copyright text-muted">
                    Copyright &copy; 小新xx
                    <br>
                    感谢支持！
                    <br>
                    <a href="http://cs.scu.edu.cn/">海纳百川</a>, 
                    <a href="http://www.tju.edu.cn/">实事求是</a>.
                </p>
            </div>
        </div>
    </div>
</footer>

    
    
        <script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.slim.min.js"></script>
        <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
        <script src="https://cdn.bootcss.com/font-awesome/5.9.0/js/all.min.js"></script>
         
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                    CommonHTML: { linebreaks: { automatic: true } },
                    "HTML-CSS": { linebreaks: { automatic: true } },
                    SVG: { linebreaks: { automatic: true } }
                });
            </script>
            <script src='https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
        
    


<script src="/js/av.min.js"></script>
<script src="/js/valine.min.js"></script>
<script src="/js/applause-easy.js"></script>

<script>
$(document).ready(function() {
    var a = new ApplauseEasy({
        id: 'applause-easy',
        appId: "xxxxxxxxxx",
        appKey: "xxxxxxxxxx",
        img_src: "http://img.hanlindong.com/blog/site/clap.png",
        img_width: "50px",
        img_height: "50px"
    })
})
</script>


<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?e8a203f609d8ced2c71c3eee859b6941";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>




    
</body>
</html>